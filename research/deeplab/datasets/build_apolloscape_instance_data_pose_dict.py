"""Converts Apolloscape car instance (pose and shape) data to TFRecord file format with Example protos.

The Apolloscape dataset is expected to have the following directory structure:

+ apolloscape
  + 3d_car_instance_sample
     + camera
     + car_models //.pkl file of car models
     + car_poses //json file annotation of pose and shape for cars in each image
     + images
     + split
     + pose_maps //pose map files generated by Apolloscape toolkit (http://yq01-sys-hic-k40-0003.yq01.baidu.com:8888/notebooks/baidu/personal-code/car-fitting/rui_modelfitting/dataset-api/car_instance/demo.ipynb)

This script converts data into sharded data files and save at tfrecord folder.

The Example proto contains the following fields:

  image/encoded: encoded image content.
  image/filename: image filename.
  image/format: image file format.
  image/height: image height.
  image/width: image width.
  image/channels: image channels.
  image/segmentation/class/encoded: encoded semantic segmentation content.
  image/segmentation/class/format: semantic segmentation file format.
"""
import glob
import math
import os.path
import re
import sys
import build_data
import tensorflow as tf
import numpy as np
np.set_printoptions(suppress=True)
import ntpath
import h5py
from scipy.io import savemat
import matplotlib.pyplot as plt
import imageio

FLAGS = tf.app.flags.FLAGS
dataset_folders = ['3d_car_instance_sample', 'full', 'combined']
tf.app.flags.DEFINE_boolean(
    'if_test',
    False,
    'False for train/val for True for test.')
if FLAGS.if_test:
    dataset_subfolders = ['', '/test', '/test']
else:
    dataset_subfolders = ['', '/train', '/train']
dataset_folder_index = 2
dataset_folder = dataset_folders[dataset_folder_index] + dataset_subfolders[dataset_folder_index]
# dataset_folder = dataset_folders[dataset_folder_index]
print '===== dataset_folder: ', dataset_folder

tf.app.flags.DEFINE_string('apolloscape_root',
                           './apolloscape/%s'%dataset_folder,
                           'Apolloscape dataset root folder.')
tf.app.flags.DEFINE_string(
    'output_dir',
    './apolloscape/%s/tfrecord_05_half'%dataset_folder,
    'Path to save converted SSTable of TensorFlow examples.')
tf.app.flags.DEFINE_string(
    'splits_dir',
    './apolloscape/%s/split'%dataset_folder,
    'Path to splits files (.txt) for train/val.')

_NUM_SHARDS = 5

# A map from data type to folder name that saves the data.
_FOLDERS_MAP = {
    'image': 'pose_maps_05',
    'seg': 'pose_maps_05',
    'shape_id_map': 'pose_maps_05',
    'vis': 'pose_maps_05',
    'depth': 'pose_maps_05',
    'pose_dict': 'pose_maps_05',
    'rotuvd_dict': 'pose_maps_05',
    'bbox_dict': 'pose_maps_05',
    'shape_id_dict': 'pose_maps_05',
}

# A map from data type to filename postfix.
_POSTFIX_MAP = {
    'image': '_rescaled_half',
    'seg': '_seg_half',
    'shape_id_map': '_shape_id_half',
    'vis': '_vis_half',
    'depth': '_depth_map_half',
    # 'image': '_rescaled',
    # 'seg': '_seg',
    # 'vis': '_vis',
    'pose_dict': '_posedict',
    'rotuvd_dict': '_rotuvddict',
    'bbox_dict': '_bboxdict',
    'shape_id_dict': '_shapeiddict'
}

# A map from data type to data format.
_DATA_FORMAT_MAP = {
    'image': 'png',
    'vis': 'png',
    'depth': 'png',
    'seg': 'png',
    'shape_id_map': 'png',
    'pose_dict': 'npy',
    'rotuvd_dict': 'npy',
    'bbox_dict': 'npy',
    'shape_id_dict': 'npy'
}

# Image file pattern.
_IMAGE_FILENAME_RE = re.compile('(.+)' + _POSTFIX_MAP['image'])

def _get_files(data, dataset_split):
  """Gets files for the specified data type and dataset split.

  Args:
    data: String, desired data ('image' or 'label').
    dataset_split: String, dataset split ('train', 'val', 'test')

  Returns:
    A list of sorted file names or None when getting label for
      test set.
  """
  print '----', '%s/%s.txt'%(FLAGS.splits_dir, dataset_split)
  text_file = open('%s/%s.txt'%(FLAGS.splits_dir, dataset_split), "r")
  filenames_split = [line.replace('.jpg', '') for line in text_file.read().split('\n') if '.jpg' in line]

  pattern = '*%s.%s' % (_POSTFIX_MAP[data], _DATA_FORMAT_MAP[data])
  search_files = os.path.join(
      FLAGS.apolloscape_root, _FOLDERS_MAP[data], pattern)
  filepaths = glob.glob(search_files)
  filepaths_split = [filepath for filepath in filepaths if ntpath.basename(filepath).replace('%s.%s' % (_POSTFIX_MAP[data], _DATA_FORMAT_MAP[data]), '') in filenames_split]
  print dataset_split, pattern, len(filepaths_split)

  return sorted(filepaths_split)


def _convert_dataset(dataset_split):
  """Converts the specified dataset split to TFRecord format.

  Args:
    dataset_split: The dataset split (e.g., train, val).

  Raises:
    RuntimeError: If loaded image and label have different shape, or if the
      image file with specified postfix could not be found.
  """
  image_files = _get_files('image', dataset_split)
  seg_files = _get_files('seg', dataset_split)
  if dataset_split != 'test':
      vis_files = _get_files('vis', dataset_split)
      depth_files = _get_files('depth', dataset_split)
      shape_id_map_files = _get_files('shape_id_map', dataset_split)
      pose_dict_files = _get_files('pose_dict', dataset_split)
      rotuvd_dict_files = _get_files('rotuvd_dict', dataset_split)
      bbox_dict_files = _get_files('bbox_dict', dataset_split)
      shape_id_dict_files = _get_files('shape_id_dict', dataset_split)

  num_images = len(image_files)
  num_per_shard = int(math.ceil(num_images / float(_NUM_SHARDS)))
  print len(image_files), len(seg_files)
  if dataset_split != 'test':
      print len(vis_files), len(pose_dict_files), len(shape_id_dict_files), len(shape_id_map_files), len(depth_files), 'Three list lengths.'
      assert len(image_files) == len(vis_files) == len(depth_files) == len(seg_files) == len(pose_dict_files) == len(rotuvd_dict_files) == len(bbox_dict_files) == len(shape_id_dict_files) == len(shape_id_map_files), 'Three list lengths not equal!'
      print 'num_images, num_segs, num_pose_dicts, num_per_shard: ', num_images, len(seg_files), len(pose_dict_files), num_per_shard
  import random
  indices = range(num_images)
  random.shuffle(indices)
  image_files = [image_files[indice] for indice in indices]
  seg_files = [seg_files[indice] for indice in indices]
  if dataset_split != 'test':
      vis_files = [vis_files[indice] for indice in indices]
      depth_files = [depth_files[indice] for indice in indices]
      shape_id_map_files = [shape_id_map_files[indice] for indice in indices]
      pose_dict_files = [pose_dict_files[indice] for indice in indices]
      rotuvd_dict_files = [rotuvd_dict_files[indice] for indice in indices]
      bbox_dict_files = [bbox_dict_files[indice] for indice in indices]
      shape_id_dict_files = [shape_id_dict_files[indice] for indice in indices]

  image_reader = build_data.ImageReader('png', channels=3)
  seg_reader = build_data.ImageReader('png', channels=1)
  if dataset_split != 'test':
      vis_reader = build_data.ImageReader('png', channels=3)
      shape_id_map_reader = build_data.ImageReader('png', channels=1)

  def return_id(filepath):
      file_name = ntpath.basename(filepath)
      splits = file_name.split('_')
      return splits[0]+'_'+splits[1]

  dims_min = np.zeros(6) + np.inf
  dims_max = np.zeros(6) - np.inf
  pose_dict_split = []

  for shard_id in range(_NUM_SHARDS):
    shard_filename = '%s-%05d-of-%05d.tfrecord' % (
        dataset_split, shard_id, _NUM_SHARDS)
    output_filename = os.path.join(FLAGS.output_dir, shard_filename)
    # options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)
    options = {}
    with tf.python_io.TFRecordWriter(output_filename, options=options) as tfrecord_writer:
      start_idx = shard_id * num_per_shard
      end_idx = min((shard_id + 1) * num_per_shard, num_images)
      for i in range(start_idx, end_idx):
        sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
            i + 1, num_images, shard_id))
        sys.stdout.flush()
        # Read the image.
        if dataset_split != 'test':
            assert return_id(image_files[i]) == return_id(vis_files[i]) == return_id(seg_files[i]) == return_id(pose_dict_files[i]) == return_id(rotuvd_dict_files[i]) == return_id(bbox_dict_files[i]), 'File name mismatch!'
        else:
            assert return_id(image_files[i]) == return_id(seg_files[i]), 'File name mismatch!'

        print image_files[i]
        image_data = tf.gfile.FastGFile(image_files[i], 'rb').read()
        height, width = image_reader.read_image_dims(image_data)
        seg_data = tf.gfile.FastGFile(seg_files[i], 'rb').read()
        height_seg, width_seg = seg_reader.read_image_dims(seg_data)
        assert height == height_seg and width == width_seg, 'W and H for image and seg must match!'
        if dataset_split != 'test':
            # Read the semantic segmentation annotation.
            vis_data = tf.gfile.FastGFile(vis_files[i], 'rb').read()
            depth_data = tf.gfile.FastGFile(depth_files[i], 'rb').read()
            # depth_data_raw = imageio.imread(depth_files[i])
            # depth_data = depth_data_raw.astype(np.float32)/256./256.*350.
            pose_dict_data = np.load(pose_dict_files[i])
            rotuvd_dict_data = np.load(rotuvd_dict_files[i])
            bbox_dict_data = np.load(bbox_dict_files[i])
            shape_id_dict_data = np.load(shape_id_dict_files[i])
            shape_id_map_data = tf.gfile.FastGFile(shape_id_map_files[i], 'rb').read()
            for instance in pose_dict_data:
                pose_dict_split.append(instance)
                for dim in range(6):
                    dims_min[dim] = min(instance[dim], dims_min[dim])
                    dims_max[dim] = max(instance[dim], dims_max[dim])
            pose_dict_data = np.vstack((np.zeros((1, 6)) + 255., pose_dict_data))
            shape_id_dict_data = np.vstack((np.zeros((1, 1)), shape_id_dict_data.reshape((-1, 1))))
            print pose_dict_data.shape, shape_id_dict_data.shape
            assert pose_dict_data.shape[0] == shape_id_dict_data.shape[0], 'pose_dict_data.shape[0] == shape_id_dict_data.shape[0]'+pose_dict_files[i]+shape_id_dict_files[i]
            assert np.min(shape_id_dict_data)>=0, 'assert np.min(shape_id_dict_data)>=0'
            assert np.max(shape_id_dict_data)<79, 'np.max(shape_id_dict_data)<79'
            assert len(shape_id_dict_data) < 255, 'len(shape_id_dict_data) < 255'

        seg_array = np.int32(plt.imread(seg_files[i]) * 255.)
        # print np.max(seg_array), np.min(seg_array)
        assert np.min(seg_array)==0, '1'
        mask_array = seg_array>0
        if dataset_split != 'test':
            shape_id_array = np.int32(plt.imread(shape_id_map_files[i]) * 255.)
            # print shape_id_array.shape, seg_array.shape
            assert np.min(shape_id_array)==0, '2'
            max_shape_id = np.max(shape_id_array[mask_array])
            print max_shape_id
            assert max_shape_id <= 79, '3'
            min_shape_id = np.min(shape_id_array[mask_array])
            print min_shape_id
            assert min_shape_id >= 1, '4'+shape_id_map_files[i]

        # Convert to tf example.
        re_match = _IMAGE_FILENAME_RE.search(image_files[i])
        if re_match is None:
          raise RuntimeError('Invalid image filename: ' + image_files[i])
        filename = '%s-%s'%(dataset_split, os.path.basename(re_match.group(1)))
        if dataset_split != 'test':
            example = build_data.image_posedict_to_tfexample(True,
                    image_data, vis_data, depth_data, seg_data, shape_id_map_data, filename,
                    height, width, pose_dict_data, rotuvd_dict_data, bbox_dict_data, shape_id_dict_data)
        else:
            example = build_data.image_posedict_to_tfexample(False,
                    image_data, None, seg_data, None, filename,
                    height, width, None, None)
        tfrecord_writer.write(example.SerializeToString())
    sys.stdout.write('\n')
    sys.stdout.flush()

  print dims_min
  print dims_max
  return pose_dict_split


def main(unused_argv):
    if FLAGS.if_test:
        _ = _convert_dataset('test')
    else:
        pose_dict_all = []
        for dataset_split in ['train', 'val']:
            pose_dict_split = _convert_dataset(dataset_split)
            pose_dict_all.append(pose_dict_split)
            # savemat('/home/zhurui/Documents/pose_dict_all.mat', {'pose_dict_all': pose_dict_all})

if __name__ == '__main__':
  tf.app.run()
